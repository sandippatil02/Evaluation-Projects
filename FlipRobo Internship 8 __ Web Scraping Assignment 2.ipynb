{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in\n",
    "“Bangalore” location. You have to scrape the job-title, job-location, company_name,\n",
    "experience_required. You have to scrape first 10 jobs data.\n",
    "\n",
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill,Designations,Companies” field and enter “Bangalore”\n",
    "in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note- All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 : Find the URL that we wish to Scrape\n",
    "https://www.naukri.com/data-analyst-jobs-in-bangalore?k=data%20analyst&l=bangalore\n",
    "\n",
    "Step 2 : Find the data you want to extract\n",
    "job-title,\n",
    "job-location,\n",
    "company_name,\n",
    "experience_required.\n",
    "\n",
    "Step 3 : Write the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define function\n",
    "def Q1():\n",
    "    # Importing essential libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "    \n",
    "    \n",
    "    # Configure the webdriver\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(\"https://www.naukri.com/data-analyst-jobs-in-bangalore?k=data%20analyst&l=bangalore\")\n",
    "    \n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    \n",
    "    # Scrapping titles from the url\n",
    "    title = soup.find_all('div', class_ = 'info fleft')\n",
    "    Job_Title = []\n",
    "    for i in title:\n",
    "        Job_Title.append(i.a.text)\n",
    "    \n",
    "    \n",
    "    # Scrapping location from the url\n",
    "    location = soup.find_all('li', class_ = 'fleft grey-text br2 placeHolderLi location')\n",
    "    Job_Location = []\n",
    "    for i in location:\n",
    "        Job_Location.append(i.text)\n",
    "        \n",
    "        # Scrapping company name from the url\n",
    "    coname = soup.find_all('div', class_ = 'mt-7 companyInfo subheading lh16')\n",
    "    Company_Name = []\n",
    "    for i in coname:\n",
    "        Company_Name.append(i.a.text)\n",
    "    \n",
    "    # Scrapping experience from the url\n",
    "    experience = soup.find_all('li', class_ = 'fleft grey-text br2 placeHolderLi experience')\n",
    "    Experience_Required = []\n",
    "    for i in experience:\n",
    "        Experience_Required.append(i.text)\n",
    "    \n",
    "    # Creating a dataframe object to store the scrapped data\n",
    "\n",
    "    DataAnalyst_Banglore = pd.DataFrame({'Job_Title' : Job_Title[0:10], 'Job_Location' : Job_Location[0:10], 'Company_Name' : Company_Name[0:10], 'Experience_Required' : Experience_Required[0:10] })\n",
    "    \n",
    "    # Returning the Dataframe\n",
    "    return DataAnalyst_Banglore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Location</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Experience_Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Immediate opening For Data Scientist/Data Analyst</td>\n",
       "      <td>Chennai, Pune, Bengaluru, Hyderabad</td>\n",
       "      <td>CAIA-Center For Artificial Intelligence &amp; Adva...</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business Analyst and Data Analyst</td>\n",
       "      <td>Delhi NCR, Bengaluru, Hyderabad</td>\n",
       "      <td>Tech Mahindra Ltd.</td>\n",
       "      <td>7-12 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Chennai, Delhi NCR, Bengaluru</td>\n",
       "      <td>Hk solutions</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Intern - DFM Data Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>GLOBALFOUNDRIES Engineering Private Limited</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hiring Data Analysts on Contract Third party p...</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Flipkart Internet Private Limited</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reliability Data Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Alstom Transport India Ltd.</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Ladder of changes</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst / Business Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Altisource</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hiring For  Data Analyst RE) - Bangalore</td>\n",
       "      <td>Bengaluru(Bellandur)</td>\n",
       "      <td>TELEPERFORMANCE GLOBAL SERVICES</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst (Telcom domain)</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Oracle India Pvt. Ltd.</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_Title  \\\n",
       "0  Immediate opening For Data Scientist/Data Analyst   \n",
       "1                  Business Analyst and Data Analyst   \n",
       "2                                       Data Analyst   \n",
       "3                          Intern - DFM Data Analyst   \n",
       "4  Hiring Data Analysts on Contract Third party p...   \n",
       "5                           Reliability Data Analyst   \n",
       "6                                       Data Analyst   \n",
       "7                    Data Analyst / Business Analyst   \n",
       "8           Hiring For  Data Analyst RE) - Bangalore   \n",
       "9                       Data Analyst (Telcom domain)   \n",
       "\n",
       "                          Job_Location  \\\n",
       "0  Chennai, Pune, Bengaluru, Hyderabad   \n",
       "1      Delhi NCR, Bengaluru, Hyderabad   \n",
       "2        Chennai, Delhi NCR, Bengaluru   \n",
       "3                            Bengaluru   \n",
       "4                            Bengaluru   \n",
       "5                            Bengaluru   \n",
       "6                            Bengaluru   \n",
       "7                            Bengaluru   \n",
       "8                 Bengaluru(Bellandur)   \n",
       "9                            Bengaluru   \n",
       "\n",
       "                                        Company_Name Experience_Required  \n",
       "0  CAIA-Center For Artificial Intelligence & Adva...             0-3 Yrs  \n",
       "1                                 Tech Mahindra Ltd.            7-12 Yrs  \n",
       "2                                       Hk solutions             0-3 Yrs  \n",
       "3        GLOBALFOUNDRIES Engineering Private Limited             0-5 Yrs  \n",
       "4                  Flipkart Internet Private Limited             2-6 Yrs  \n",
       "5                        Alstom Transport India Ltd.             3-8 Yrs  \n",
       "6                                  Ladder of changes             0-5 Yrs  \n",
       "7                                         Altisource             1-6 Yrs  \n",
       "8                    TELEPERFORMANCE GLOBAL SERVICES             2-6 Yrs  \n",
       "9                             Oracle India Pvt. Ltd.             3-8 Yrs  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in\n",
    "“Bangalore” location. You have to scrape the job-title, job-location,\n",
    "company_name, full job-description. You have to scrape first 10 jobs data.\n",
    "\n",
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field and enter\n",
    "“Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note- 1. All of the above steps have to be done in code. No step is to be done\n",
    "manually.\n",
    "2.Please note that you have to scrape full job description. For that you may have to\n",
    "open each job separately as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 : Find the URL that we wish to Scrape\n",
    "https://www.naukri.com/data-analyst-jobs-in-bangalore?k=data%20analyst&l=bangalore\n",
    "\n",
    "Step 2 : Find the data you want to extract\n",
    "job-title,\n",
    "job-location,\n",
    "company_name,\n",
    "full job-description.\n",
    "\n",
    "Step 3 : Write the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function\n",
    "def Q2():\n",
    "    \n",
    "    # Importing essential libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    \n",
    "    # Configuring the url\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(\"https://www.naukri.com/data-analyst-jobs-in-bangalore?k=data%20analyst&l=bangalore\")\n",
    "    \n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    # Scraping titles\n",
    "    title = soup.find_all('div', class_ = 'info fleft')\n",
    "    Job_Title = []\n",
    "    for i in title:\n",
    "        Job_Title.append(i.a.text)\n",
    "\n",
    "    \n",
    "    # Scaping locations\n",
    "    location = soup.find_all('li', class_ = 'fleft grey-text br2 placeHolderLi location')\n",
    "    Job_Location = []\n",
    "    for i in location:\n",
    "        Job_Location.append(i.text)\n",
    "    \n",
    "    \n",
    "    # Scraping company names\n",
    "    coname = soup.find_all('div', class_ = 'mt-7 companyInfo subheading lh16')\n",
    "    Company_Name = []\n",
    "    for i in coname:\n",
    "        Company_Name.append(i.a.text)\n",
    "        \n",
    "        \n",
    "    # Creating urls to scrape full descriptions   \n",
    "    desc_url1 = \"https://www.naukri.com/job-listings-immediate-opening-for-data-scientist-data-analyst-caia-center-for-artificial-intelligence-advanced-analytics-chennai-pune-bengaluru-bangalore-hyderabad-secunderabad-0-to-3-years-210920000599?src=jobsearchDesk&sid=16043158234975318&xp=1&px=1\"\n",
    "    desc_url2 = \"https://www.naukri.com/job-listings-senior-data-analyst-myntra-designs-pvt-ltd-bengaluru-bangalore-2-to-7-years-281020901652?src=jobsearchDesk&sid=16043158234975318&xp=2&px=1\"\n",
    "    desc_url3 = \"https://www.naukri.com/job-listings-senior-sourcing-data-analyst-coupa-erp-jaggaer-technicolor-bengaluru-bangalore-3-to-10-years-211020501122?src=jobsearchDesk&sid=16043158234975318&xp=3&px=1\"\n",
    "    desc_url4 = \"https://www.naukri.com/job-listings-data-analyst-ladder-of-changes-bengaluru-bangalore-0-to-5-years-311020900299?src=jobsearchDesk&sid=16043158234975318&xp=4&px=1\"\n",
    "    desc_url5 = \"https://www.naukri.com/job-listings-data-analyst-business-analyst-altisource-business-solutions-pvt-ltd-bengaluru-bangalore-1-to-6-years-291020005716?src=jobsearchDesk&sid=16043158234975318&xp=5&px=1\"\n",
    "    desc_url6 = \"https://www.naukri.com/job-listings-hiring-for-data-analyst-re-bangalore-teleperformance-global-services-pvt-ltd-bengaluru-bangalore-2-to-6-years-281020004246?src=jobsearchDesk&sid=16043158234975318&xp=6&px=1\"\n",
    "    desc_url7 = \"https://www.naukri.com/job-listings-data-analyst-pricing-zscaler-inc-pune-bengaluru-bangalore-chandigarh-3-to-6-years-191020501453?src=jobsearchDesk&sid=16043158234975318&xp=7&px=1\"\n",
    "    desc_url8 = \"https://www.naukri.com/job-listings-data-analyst-omega-seiki-chennai-pune-delhi-ncr-mumbai-ahmedabad-bengaluru-bangalore-surat-hyderabad-secunderabad-kolkata-0-to-5-years-311020002972?src=jobsearchDesk&sid=16043158234975318&xp=8&px=1\"\n",
    "    desc_url9 = \"https://www.naukri.com/job-listings-manager-data-analyst-brand-accelerator-myntra-designs-pvt-ltd-bengaluru-bangalore-2-to-7-years-231020900323?src=jobsearchDesk&sid=16043158234975318&xp=9&px=1\"\n",
    "    desc_url10 = \"https://www.naukri.com/job-listings-data-analyst-snaphunt-bengaluru-bangalore-3-to-5-years-030920000554?src=jobsearchDesk&sid=16043158234975318&xp=10&px=1\"\n",
    "    desc_url11 = \"https://www.naukri.com/job-listings-data-analyst-simplify360-bengaluru-bangalore-4-to-5-years-291020500625?src=jobsearchDesk&sid=16043158234975318&xp=11&px=1\"\n",
    "    desc_url12 = \"https://www.naukri.com/job-listings-data-analyst-and-dashboard-developer-qualitest-group-bengaluru-bangalore-2-to-5-years-221020501006?src=jobsearchDesk&sid=16043158234975318&xp=12&px=1\"\n",
    "    desc_url13 = \"https://www.naukri.com/job-listings-data-analyst-liventus-inc-bengaluru-bangalore-3-to-6-years-150720003694?src=jobsearchDesk&sid=16043158234975318&xp=13&px=1\"\n",
    "    desc_url14 = \"https://www.naukri.com/job-listings-business-analyst-data-analyst-dotsolved-india-pvt-ltd-chennai-delhi-ncr-bengaluru-bangalore-8-to-13-years-091020003800?src=jobsearchDesk&sid=16043158234975318&xp=14&px=1\"\n",
    "    desc_url15 = \"https://www.naukri.com/job-listings-business-data-analyst-kaplan-bengaluru-bangalore-2-to-5-years-301020501172?src=jobsearchDesk&sid=16043158234975318&xp=15&px=1\"\n",
    "    desc_url16 = \"https://www.naukri.com/job-listings-business-data-analyst-kaplan-test-prep-bengaluru-bangalore-2-to-5-years-301020500108?src=jobsearchDesk&sid=16043158234975318&xp=16&px=1\"\n",
    "    desc_url17 = \"https://www.naukri.com/job-listings-data-analyst-intertrustviteos-corporate-and-fund-services-pvt-ltd-chennai-mumbai-bengaluru-bangalore-7-to-10-years-301020004142?src=jobsearchDesk&sid=16043158234975318&xp=17&px=1\"\n",
    "    desc_url18 = \"https://www.naukri.com/job-listings-senior-data-analyst-python-sql-mysql-serving-skill-bengaluru-bangalore-6-to-9-years-111219901925?src=jobsearchDesk&sid=16043158234975318&xp=18&px=1\"\n",
    "    desc_url19 = \"https://www.naukri.com/job-listings-senior-clinical-data-analyst-navitas-life-science-chennai-bengaluru-bangalore-2-to-5-years-231020500044?src=jobsearchDesk&sid=16043158234975318&xp=19&px=1\"\n",
    "    desc_url20 = \"https://www.naukri.com/job-listings-clinical-data-analyst-navitas-life-science-chennai-bengaluru-bangalore-1-to-4-years-231020500042?src=jobsearchDesk&sid=16043158234975318&xp=20&px=1\"\n",
    "        \n",
    "    # Creating a dictionary to store the description    \n",
    "    desc = [desc_url1, desc_url2, desc_url3, desc_url4, desc_url5, desc_url6, desc_url7, desc_url8, desc_url9, desc_url10, desc_url11, desc_url12, desc_url13, desc_url14, desc_url15, desc_url16, desc_url17, desc_url18, desc_url19, desc_url20]\n",
    "    \n",
    "    \n",
    "    # Scraping descriptons\n",
    "    \n",
    "    driver.get(desc_url1)\n",
    "    content = driver.page_source\n",
    "    soup1 = BeautifulSoup(content)\n",
    "    job_desc = soup1.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description1 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description1.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url2)\n",
    "    content = driver.page_source\n",
    "    soup2 = BeautifulSoup(content)\n",
    "    job_desc = soup2.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description2 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description2.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url3)\n",
    "    content = driver.page_source\n",
    "    soup3 = BeautifulSoup(content)\n",
    "    job_desc = soup3.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description3 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description3.append(i.text)\n",
    "\n",
    "    driver.get(desc_url4)\n",
    "    content = driver.page_source\n",
    "    soup4 = BeautifulSoup(content)\n",
    "    job_desc = soup4.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description4 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description4.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url5)\n",
    "    content = driver.page_source\n",
    "    soup5 = BeautifulSoup(content)\n",
    "    job_desc = soup5.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description5 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description5.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url6)\n",
    "    content = driver.page_source\n",
    "    soup6 = BeautifulSoup(content)\n",
    "    job_desc = soup6.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description6 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description6.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url7)\n",
    "    content = driver.page_source\n",
    "    soup7 = BeautifulSoup(content)\n",
    "    job_desc = soup7.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description7 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description7.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url8)\n",
    "    content = driver.page_source\n",
    "    soup8 = BeautifulSoup(content)\n",
    "    job_desc = soup8.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description8 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description8.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url9)\n",
    "    content = driver.page_source\n",
    "    soup9 = BeautifulSoup(content)\n",
    "    job_desc = soup9.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description9 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description9.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url10)\n",
    "    content = driver.page_source\n",
    "    soup10 = BeautifulSoup(content)\n",
    "    job_desc = soup10.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description10 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description10.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url11)\n",
    "    content = driver.page_source\n",
    "    soup11 = BeautifulSoup(content)\n",
    "    job_desc = soup11.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description11 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description11.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url12)\n",
    "    content = driver.page_source\n",
    "    soup12 = BeautifulSoup(content)\n",
    "    job_desc = soup12.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description12 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description12.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url13)\n",
    "    content = driver.page_source\n",
    "    soup13 = BeautifulSoup(content)\n",
    "    job_desc = soup13.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description13 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description13.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url14)\n",
    "    content = driver.page_source\n",
    "    soup14 = BeautifulSoup(content)\n",
    "    job_desc = soup14.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description14 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description14.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url15)\n",
    "    content = driver.page_source\n",
    "    soup15 = BeautifulSoup(content)\n",
    "    job_desc = soup15.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description15 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description15.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url16)\n",
    "    content = driver.page_source\n",
    "    soup16 = BeautifulSoup(content)\n",
    "    job_desc = soup16.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description16 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description16.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url17)\n",
    "    content = driver.page_source\n",
    "    soup17 = BeautifulSoup(content)\n",
    "    job_desc = soup17.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description17 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description17.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url18)\n",
    "    content = driver.page_source\n",
    "    soup18 = BeautifulSoup(content)\n",
    "    job_desc = soup18.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description18 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description18.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url19)\n",
    "    content = driver.page_source\n",
    "    soup19 = BeautifulSoup(content)\n",
    "    job_desc = soup19.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description19 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description19.append(i.text)\n",
    "        \n",
    "    driver.get(desc_url20)\n",
    "    content = driver.page_source\n",
    "    soup20 = BeautifulSoup(content)\n",
    "    job_desc = soup20.find_all('section', class_ = 'job-desc')\n",
    "    Job_Description20 = []\n",
    "    for i in job_desc:\n",
    "        Job_Description20.append(i.text)\n",
    "        \n",
    "    # Appending all the scrapped descriptions\n",
    "    JD = Job_Description1+Job_Description2+Job_Description3+Job_Description4+Job_Description5+Job_Description6+Job_Description7+Job_Description8+Job_Description9+Job_Description10+Job_Description11+Job_Description12+Job_Description13+Job_Description14+Job_Description15+Job_Description16+Job_Description17+Job_Description18+Job_Description19+Job_Description20\n",
    "    \n",
    "    \n",
    "    # Creating a dataframe to store the scrapped information\n",
    "    Naukri_listng = pd.DataFrame({'Job_Title' : Job_Title[0:10], 'Job_Location' : Job_Location[0:10], 'Company_Name' : Company_Name[0:10], 'Job_Description' : JD[0:10]})\n",
    "    \n",
    "    # Returning the dataframe\n",
    "    return Naukri_listng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Location</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Job_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Immediate opening For Data Scientist/Data Analyst</td>\n",
       "      <td>Chennai, Pune, Bengaluru, Hyderabad</td>\n",
       "      <td>CAIA-Center For Artificial Intelligence &amp; Adva...</td>\n",
       "      <td>Job description Dear Candidate  Schedule a Tel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business Analyst and Data Analyst</td>\n",
       "      <td>Delhi NCR, Bengaluru, Hyderabad</td>\n",
       "      <td>Tech Mahindra Ltd.</td>\n",
       "      <td>Job description     Key roles of this position...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Chennai, Delhi NCR, Bengaluru</td>\n",
       "      <td>Hk solutions</td>\n",
       "      <td>Job descriptionRoles and ResponsibilitiesRespo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Intern - DFM Data Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>GLOBALFOUNDRIES Engineering Private Limited</td>\n",
       "      <td>Job descriptionGreetings from “Altisource busi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hiring Data Analysts on Contract Third party p...</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Flipkart Internet Private Limited</td>\n",
       "      <td>Job descriptionHiring for Data Analyst(RE)Qual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reliability Data Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Alstom Transport India Ltd.</td>\n",
       "      <td>Job description We?re looking for a data analy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Ladder of changes</td>\n",
       "      <td>Job descriptionRoles and ResponsibilitiesRespo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst / Business Analyst</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Altisource</td>\n",
       "      <td>Job description The Offer Join an exciting, aw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Hiring For  Data Analyst RE) - Bangalore</td>\n",
       "      <td>Bengaluru(Bellandur)</td>\n",
       "      <td>TELEPERFORMANCE GLOBAL SERVICES</td>\n",
       "      <td>Job description Responsibilities:   ? Managing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst (Telcom domain)</td>\n",
       "      <td>Bengaluru</td>\n",
       "      <td>Oracle India Pvt. Ltd.</td>\n",
       "      <td>Job description  Job description     Any Datab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_Title  \\\n",
       "0  Immediate opening For Data Scientist/Data Analyst   \n",
       "1                  Business Analyst and Data Analyst   \n",
       "2                                       Data Analyst   \n",
       "3                          Intern - DFM Data Analyst   \n",
       "4  Hiring Data Analysts on Contract Third party p...   \n",
       "5                           Reliability Data Analyst   \n",
       "6                                       Data Analyst   \n",
       "7                    Data Analyst / Business Analyst   \n",
       "8           Hiring For  Data Analyst RE) - Bangalore   \n",
       "9                       Data Analyst (Telcom domain)   \n",
       "\n",
       "                          Job_Location  \\\n",
       "0  Chennai, Pune, Bengaluru, Hyderabad   \n",
       "1      Delhi NCR, Bengaluru, Hyderabad   \n",
       "2        Chennai, Delhi NCR, Bengaluru   \n",
       "3                            Bengaluru   \n",
       "4                            Bengaluru   \n",
       "5                            Bengaluru   \n",
       "6                            Bengaluru   \n",
       "7                            Bengaluru   \n",
       "8                 Bengaluru(Bellandur)   \n",
       "9                            Bengaluru   \n",
       "\n",
       "                                        Company_Name  \\\n",
       "0  CAIA-Center For Artificial Intelligence & Adva...   \n",
       "1                                 Tech Mahindra Ltd.   \n",
       "2                                       Hk solutions   \n",
       "3        GLOBALFOUNDRIES Engineering Private Limited   \n",
       "4                  Flipkart Internet Private Limited   \n",
       "5                        Alstom Transport India Ltd.   \n",
       "6                                  Ladder of changes   \n",
       "7                                         Altisource   \n",
       "8                    TELEPERFORMANCE GLOBAL SERVICES   \n",
       "9                             Oracle India Pvt. Ltd.   \n",
       "\n",
       "                                     Job_Description  \n",
       "0  Job description Dear Candidate  Schedule a Tel...  \n",
       "1  Job description     Key roles of this position...  \n",
       "2  Job descriptionRoles and ResponsibilitiesRespo...  \n",
       "3  Job descriptionGreetings from “Altisource busi...  \n",
       "4  Job descriptionHiring for Data Analyst(RE)Qual...  \n",
       "5  Job description We?re looking for a data analy...  \n",
       "6  Job descriptionRoles and ResponsibilitiesRespo...  \n",
       "7  Job description The Offer Join an exciting, aw...  \n",
       "8  Job description Responsibilities:   ? Managing...  \n",
       "9  Job description  Job description     Any Datab...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using the filters available on the\n",
    "webpage as shown below:\n",
    "    \n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company_name,\n",
    "experience_required.\n",
    "The location filter to be used is “Delhi/NCR”\n",
    "The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field .\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note- All of the above steps have to be done in code. No step is to be done\n",
    "manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 : Find the URL that we wish to Scrape\n",
    "https://www.naukri.com/data-scientist-jobs?k=data%20scientist&ctcFilter=3to6&cityType=25.9.31\n",
    "\n",
    "Step 2 : Find the data you want to extract\n",
    "job-title,\n",
    "job-location,\n",
    "company_name,\n",
    "experience_required.\n",
    "\n",
    "Step 3 : Write the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q3():\n",
    "    # Importing essential libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "    \n",
    "    \n",
    "    # Configure the webdriver\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(\"https://www.naukri.com/data-scientist-jobs?k=data%20scientist&ctcFilter=3to6&cityType=25.9.31\")\n",
    "    \n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    \n",
    "    # Scrapping titles from the url\n",
    "    title = soup.find_all('div', class_ = 'info fleft')\n",
    "    Job_Title = []\n",
    "    for i in title:\n",
    "        Job_Title.append(i.a.text)\n",
    "    \n",
    "    \n",
    "    # Scrapping location from the url\n",
    "    location = soup.find_all('li', class_ = 'fleft grey-text br2 placeHolderLi location')\n",
    "    Job_Location = []\n",
    "    for i in location:\n",
    "        Job_Location.append(i.text)\n",
    "    \n",
    "    \n",
    "    # Scrapping company name from the url\n",
    "    coname = soup.find_all('div', class_ = 'mt-7 companyInfo subheading lh16')\n",
    "    Company_Name = []\n",
    "    for i in coname:\n",
    "        Company_Name.append(i.a.text)\n",
    "    \n",
    "    # Scrapping experience from the url\n",
    "    experience = soup.find_all('li', class_ = 'fleft grey-text br2 placeHolderLi experience')\n",
    "    Experience_Required = []\n",
    "    for i in experience:\n",
    "        Experience_Required.append(i.text)\n",
    "    \n",
    "    # Creating a dataframe object to store the scrapped data\n",
    "\n",
    "    Dat_ascientist_DelhiNCR = pd.DataFrame({'Job_Title' : Job_Title[0:10], 'Job_Location' : Job_Location[0:10], 'Company_Name' : Company_Name[0:10], 'Experience_Required' : Experience_Required[0:10] })\n",
    "    \n",
    "    # Returning the Dataframe\n",
    "    return Dat_ascientist_DelhiNCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_Title</th>\n",
       "      <th>Job_Location</th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Experience_Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Only Fresher / Data Scientist / Data Analyst /...</td>\n",
       "      <td>Delhi NCR, Ghaziabad, Gurgaon</td>\n",
       "      <td>GABA Consultancy services</td>\n",
       "      <td>0-0 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist/Analyst - Machine Learning/Deep...</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>EchoIndia</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Blue Sky Analytics</td>\n",
       "      <td>1-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist - NLP/ML/Python</td>\n",
       "      <td>Gurgaon Gurugram</td>\n",
       "      <td>Elixir Web Solutions</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist - NLP/ML/Python</td>\n",
       "      <td>Gurgaon Gurugram</td>\n",
       "      <td>Elixir Web Solutions</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>GlobalHunt India Private Limited</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist/BPM/4-8 years</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Crescendo global services</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst/Scientist</td>\n",
       "      <td>Delhi NCR, Ghaziabad</td>\n",
       "      <td>Amity University</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist Machine Learning</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>Delhivery</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Business Analyst - Data Scientist</td>\n",
       "      <td>Gurgaon</td>\n",
       "      <td>HyreFox Consultants Pvt Ltd</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job_Title  \\\n",
       "0  Only Fresher / Data Scientist / Data Analyst /...   \n",
       "1  Data Scientist/Analyst - Machine Learning/Deep...   \n",
       "2                                     Data Scientist   \n",
       "3                    Data Scientist - NLP/ML/Python    \n",
       "4                    Data Scientist - NLP/ML/Python    \n",
       "5                                     Data Scientist   \n",
       "6                       Data Scientist/BPM/4-8 years   \n",
       "7                             Data Analyst/Scientist   \n",
       "8                    Data Scientist Machine Learning   \n",
       "9                  Business Analyst - Data Scientist   \n",
       "\n",
       "                    Job_Location                       Company_Name  \\\n",
       "0  Delhi NCR, Ghaziabad, Gurgaon         GABA Consultancy services    \n",
       "1                          Delhi                          EchoIndia   \n",
       "2                        Gurgaon                 Blue Sky Analytics   \n",
       "3               Gurgaon Gurugram               Elixir Web Solutions   \n",
       "4               Gurgaon Gurugram               Elixir Web Solutions   \n",
       "5                          Delhi  GlobalHunt India Private Limited    \n",
       "6                        Gurgaon          Crescendo global services   \n",
       "7           Delhi NCR, Ghaziabad                   Amity University   \n",
       "8                        Gurgaon                          Delhivery   \n",
       "9                        Gurgaon        HyreFox Consultants Pvt Ltd   \n",
       "\n",
       "  Experience_Required  \n",
       "0             0-0 Yrs  \n",
       "1             3-6 Yrs  \n",
       "2             1-6 Yrs  \n",
       "3             4-8 Yrs  \n",
       "4             4-8 Yrs  \n",
       "5             3-6 Yrs  \n",
       "6             4-7 Yrs  \n",
       "7             6-8 Yrs  \n",
       "8             1-3 Yrs  \n",
       "9             3-5 Yrs  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Write a python program to scrape data for first 10 job results for Data scientist\n",
    "Designation in Noida location. You have to scrape company_name, No. of days\n",
    "ago when job was posted, Rating of the company.\n",
    "\n",
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.glassdoor.co.in/index.htm\n",
    "2. Enter “Data Scientist” in “Job Title,Keyword,Company” field and enter “Noida”\n",
    "in “location” field.\n",
    "3. Then click the search button. You will land up in the below page:\n",
    "4. Then scrape the data for the first 10 jobs results you get in the above shown\n",
    "page.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "Note- All of the above steps have to be done in code. No step is to be done\n",
    "manually.\n",
    "\n",
    "Step 1 : Find the URL that we wish to Scrape\n",
    "https://www.glassdoor.co.in/Job/noida-data-scientist-jobs-SRCH_IL.0,5_IC4477468_KO6,20.htm\n",
    "\n",
    "Step 2 : Find the data you want to extract\n",
    "company_name,\n",
    "No. of days ago when job was posted,\n",
    "Rating of the company.\n",
    "\n",
    "Step 3 : Write the code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q4():\n",
    "    \n",
    "\n",
    "# Importing essential libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "    \n",
    "# Configure the webdriver\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(\"https://www.glassdoor.co.in/Job/noida-data-scientist-jobs-SRCH_IL.0,5_IC4477468_KO6,20.htm\")\n",
    "    \n",
    "# Creating soup\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    co_name = soup.find_all('div', class_ = 'jobHeader d-flex justify-content-between align-items-start')\n",
    "\n",
    "    Company_Name = []\n",
    "\n",
    "    for i in co_name:\n",
    "        Company_Name.append(i.text)\n",
    "        \n",
    "        \n",
    "    job_posted = soup.find_all('div', class_ = 'd-flex align-items-end pl-std css-mi55ob')\n",
    "\n",
    "    Posted_Days_Ago = []\n",
    "\n",
    "    for i in job_posted:\n",
    "        Posted_Days_Ago.append(i.text)\n",
    "        \n",
    "        \n",
    "    rating = soup.find_all('span', class_ = 'compactStars')\n",
    "\n",
    "    Rating = []\n",
    "\n",
    "    for i in rating:\n",
    "        Rating.append(i.text)\n",
    "        \n",
    "        \n",
    "        \n",
    "# Creating a dataframe object to store the scrapped data\n",
    "\n",
    "    Glassdor_Noida = pd.DataFrame({'Company_Name' : Company_Name[0:10], 'Posted_Days_Ago' : Posted_Days_Ago[0:10], 'Rating' : Rating[0:10]})\n",
    "    \n",
    "# Returning the Dataframe\n",
    "    return Glassdor_Noida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company_Name</th>\n",
       "      <th>Posted_Days_Ago</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ericsson-Worldwide</td>\n",
       "      <td>5d</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>American Express Global Business Travel</td>\n",
       "      <td>25d</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Genpact</td>\n",
       "      <td>18d</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adobe</td>\n",
       "      <td>20d</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brickred</td>\n",
       "      <td>9d</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GroundTruth</td>\n",
       "      <td>6d</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Genpact</td>\n",
       "      <td>23d</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Algoscale</td>\n",
       "      <td>18d</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Adobe</td>\n",
       "      <td>20d</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Valiance Solutions</td>\n",
       "      <td>18d</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Company_Name Posted_Days_Ago Rating\n",
       "0                       Ericsson-Worldwide              5d      4\n",
       "1  American Express Global Business Travel             25d    3.7\n",
       "2                                  Genpact             18d    3.8\n",
       "3                                    Adobe             20d    4.3\n",
       "4                                 Brickred              9d    3.7\n",
       "5                              GroundTruth              6d    3.3\n",
       "6                                  Genpact             23d    3.8\n",
       "7                                Algoscale             18d    3.7\n",
       "8                                    Adobe             20d    4.3\n",
       "9                       Valiance Solutions             18d      4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: Write a python program to scrape the salary data for Data Scientist designation\n",
    "in Noida location.\n",
    "You have to scrape Company name, Number of salaries, Average salary, Min\n",
    "salary, Max Salary.\n",
    "The above task will be, done as shown in the below steps:\n",
    "1. first get the webpage https://www.glassdoor.co.in/Salaries/index.htm\n",
    "2. Enter “Data Scientist” in Job title field and “Noida” in location field.\n",
    "3. Click the search button.\n",
    "4. After that you will land on the below page.\n",
    "You have to scrape whole data from this webpage\n",
    "5. Scrape data for first 10 companies. Scrape the min salary, max salary, company\n",
    "name, Average salary and rating of the company.\n",
    "6.Store the data in a dataframe.\n",
    "\n",
    "Note that all of the above steps have to be done by coding only and not manually.\n",
    "\n",
    "Step 1 : Find the URL that we wish to Scrape\n",
    "https://www.glassdoor.co.in/Salaries/new-delhi-data-scientist-salary-SRCH_IL.0,9_IM1083_KO10,24.htm\n",
    "\n",
    "Step 2 : Find the data you want to extract\n",
    "Company name,\n",
    "Number of salaries,\n",
    "Average salary,\n",
    "Min salary,\n",
    "Max Salary.\n",
    "\n",
    "Step 3 : Write the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q5():\n",
    "    \n",
    "    # Importing essential libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "\n",
    "    # Configure the webdriver\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(\"https://www.glassdoor.co.in/Salaries/new-delhi-data-scientist-salary-SRCH_IL.0,9_IM1083_KO10,24.htm\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    \n",
    "    #min_salary = soup.find_all('div', class_ = 'common__RangeBarStyle__values common__flex__justifySpaceBetween common__flex__container')\n",
    "    #Min_Salary = []\n",
    "    #for i in min_salary:\n",
    "    #    Min_Salary.append(i.span.text)\n",
    "    #len(Min_Salary)\n",
    "    \n",
    "    # scrapping salaries\n",
    "    min_max_salary = soup.find_all('div', class_ = 'common__RangeBarStyle__values common__flex__justifySpaceBetween common__flex__container')\n",
    "    Min_Max_Salary = []\n",
    "    for i in min_max_salary:\n",
    "        Min_Max_Salary.append(i.text)\n",
    "        \n",
    "    # Scrapping average salary\n",
    "    avg_sal = soup.find_all('div', class_ = 'col-2 d-none d-md-flex flex-row justify-content-end')\n",
    "    Average_Salary = []\n",
    "    for i in avg_sal:\n",
    "        Average_Salary.append(i.strong.text)\n",
    "        \n",
    "    # Scrapping no. of salaries    \n",
    "    no_sal = soup.find_all('p', class_ = 'css-1uyte9r css-1kuy7z7 m-0')\n",
    "    No_of_Salaries = []\n",
    "    for i in no_sal:\n",
    "        No_of_Salaries.append(i.text)\n",
    "        \n",
    "    # Scrapping the name of the company    \n",
    "    name = soup.find_all('p', class_ = 'm-0')\n",
    "    access_dict = [2,8,14,20,26,32,38,44,50,56,62,68,74,80,86,92,98,104,110,116]\n",
    "    Company_name = []\n",
    "    for i in access_dict:\n",
    "        Company_name.append(name[i].text)\n",
    "        \n",
    "    # Returning the dataframe    \n",
    "    Glassdoor_Noida = pd.DataFrame({'Company_name' : Company_name[0:10], 'No_of_Salaries' : No_of_Salaries[0:10], 'Average_Salary' : Average_Salary[0:10], 'Min_Max_Salary' : Min_Max_Salary[0:10]})\n",
    "    return Glassdoor_Noida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company_name</th>\n",
       "      <th>No_of_Salaries</th>\n",
       "      <th>Average_Salary</th>\n",
       "      <th>Min_Max_Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Delhivery</td>\n",
       "      <td>11 salaries</td>\n",
       "      <td>₹ 13,18,563</td>\n",
       "      <td>₹706K₹11,513K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Accenture</td>\n",
       "      <td>8 salaries</td>\n",
       "      <td>₹ 9,85,497</td>\n",
       "      <td>₹572K₹1,300K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IBM</td>\n",
       "      <td>7 salaries</td>\n",
       "      <td>₹ 7,53,602</td>\n",
       "      <td>₹581K₹2,704K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UnitedHealth Group</td>\n",
       "      <td>7 salaries</td>\n",
       "      <td>₹ 13,23,634</td>\n",
       "      <td>₹710K₹1,559K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cognizant Technology Solutions</td>\n",
       "      <td>6 salaries</td>\n",
       "      <td>₹ 9,97,979</td>\n",
       "      <td>₹785K₹1,251K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Valiance Solutions</td>\n",
       "      <td>6 salaries</td>\n",
       "      <td>₹ 7,72,507</td>\n",
       "      <td>₹497K₹1,140K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Vidooly Media Tech</td>\n",
       "      <td>6 salaries</td>\n",
       "      <td>₹ 12,689</td>\n",
       "      <td>₹8K₹20K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Analytics Vidhya</td>\n",
       "      <td>6 salaries</td>\n",
       "      <td>₹ 21,215</td>\n",
       "      <td>₹14K₹22K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Tata Consultancy Services</td>\n",
       "      <td>5 salaries</td>\n",
       "      <td>₹ 6,77,498</td>\n",
       "      <td>₹480K₹1,000K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ericsson-Worldwide</td>\n",
       "      <td>5 salaries</td>\n",
       "      <td>₹ 7,34,456</td>\n",
       "      <td>₹460K₹1,598K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Company_name No_of_Salaries Average_Salary Min_Max_Salary\n",
       "0                       Delhivery    11 salaries    ₹ 13,18,563  ₹706K₹11,513K\n",
       "1                       Accenture     8 salaries     ₹ 9,85,497   ₹572K₹1,300K\n",
       "2                             IBM     7 salaries     ₹ 7,53,602   ₹581K₹2,704K\n",
       "3              UnitedHealth Group     7 salaries    ₹ 13,23,634   ₹710K₹1,559K\n",
       "4  Cognizant Technology Solutions     6 salaries     ₹ 9,97,979   ₹785K₹1,251K\n",
       "5              Valiance Solutions     6 salaries     ₹ 7,72,507   ₹497K₹1,140K\n",
       "6              Vidooly Media Tech     6 salaries       ₹ 12,689        ₹8K₹20K\n",
       "7                Analytics Vidhya     6 salaries       ₹ 21,215       ₹14K₹22K\n",
       "8       Tata Consultancy Services     5 salaries     ₹ 6,77,498   ₹480K₹1,000K\n",
       "9              Ericsson-Worldwide     5 salaries     ₹ 7,34,456   ₹460K₹1,598K"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6 : Scrape data of first 100 sunglasses listings on flipkart.com. You have to\n",
    "scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. Discount %\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to flipkart webpage by url https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and\n",
    "more” is written and click the search icon\n",
    "3. after that you will reach to a webpage having a lot of sunglasses. From this page\n",
    "you can scrap the required data as usual.\n",
    "4. after scraping data from the first page, go to the “Next” Button at the bottom of\n",
    "the page , then click on it\n",
    "5. Now scrape data from this page as usual\n",
    "6. repeat this until you get data for 100 sunglasses.\n",
    "\n",
    "Note that all of the above steps have to be done by coding only and not manually.\n",
    "\n",
    "Step 1 : Find the URL that we wish to Scrape\n",
    "https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off\n",
    "\n",
    "Step 2 : Find the data you want to extract\n",
    "Brand,\n",
    "Product Description,\n",
    "Price,\n",
    "Discount %.\n",
    "\n",
    "Step 3 : Write the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q6():\n",
    "    \n",
    "    # Importing essential libraries\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "\n",
    "    # Configure the webdriver\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(\"https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    brand = soup.find_all('div', class_ = '_2B_pmu')\n",
    "    Brand = []\n",
    "    for i in brand:\n",
    "        Brand.append(i.text[0:40])\n",
    "\n",
    "    # Scraping description of the product\n",
    "    description = soup.find_all('a', class_ = '_2mylT6')\n",
    "    Product_Description = []\n",
    "    for i in description:\n",
    "        Product_Description.append(i.text)\n",
    "\n",
    "    # Scraping prce of the product\n",
    "    price = soup.find_all('div', class_ = '_1vC4OE')\n",
    "    Price = []\n",
    "    for i in price:\n",
    "        Price.append(i.text[0:40])\n",
    "\n",
    "    # Scraping discount on the product\n",
    "    discount = soup.find_all('div', class_ = 'VGWI6T')\n",
    "    Discount = []\n",
    "    for i in discount:\n",
    "        Discount.append(i)\n",
    "\n",
    "    \n",
    "    # Scaping next page for more results\n",
    "    driver.get(\"https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=2\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup1 = BeautifulSoup(content)\n",
    "    \n",
    "    # Scraping brand of the products - page 2\n",
    "    brand1 = soup1.find_all('div', class_ = '_2B_pmu')\n",
    "    Brand1 = []\n",
    "    for i in brand1:\n",
    "        Brand1.append(i.text[0:40])\n",
    "\n",
    "    # Sctraping description of the products - page 2\n",
    "    description1 = soup1.find_all('a', class_ = '_2mylT6')\n",
    "    Product_Description1 = []\n",
    "    for i in description1:\n",
    "        Product_Description1.append(i.text)\n",
    "\n",
    "    # Scraping price of the products - page 2\n",
    "    price1 = soup1.find_all('div', class_ = '_1vC4OE')\n",
    "    Price1 = []\n",
    "    for i in price1:\n",
    "        Price1.append(i.text[0:40])\n",
    "\n",
    "    # Scraping discount on the products - page 2\n",
    "    discount1 = soup1.find_all('div', class_ = 'VGWI6T')\n",
    "    Discount1 = []\n",
    "    for i in discount1:\n",
    "        Discount1.append(i)\n",
    "\n",
    "    # Scraping next page for more results\n",
    "    driver.get(\"https://www.flipkart.com/search?q=sunglasses&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=off&as=off&page=3\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup2 = BeautifulSoup(content)\n",
    "    \n",
    "    # Scraping brand of the product - page 3\n",
    "    brand2 = soup2.find_all('div', class_ = '_2B_pmu')\n",
    "    Brand2 = []\n",
    "    for i in brand2:\n",
    "        Brand2.append(i.text[0:40])\n",
    "\n",
    "    # Scraping description of the product - page 3\n",
    "    description2 = soup2.find_all('a', class_ = '_2mylT6')\n",
    "    Product_Description2 = []\n",
    "    for i in description2:\n",
    "        Product_Description2.append(i.text)\n",
    "\n",
    "    # Scraping price of the product - page 3\n",
    "    price2 = soup2.find_all('div', class_ = '_1vC4OE')\n",
    "    Price2 = []\n",
    "    for i in price2:\n",
    "        Price2.append(i.text[0:40])\n",
    "    \n",
    "    # Scraping discounts on the products - page 3\n",
    "    discount2 = soup2.find_all('div', class_ = 'VGWI6T')\n",
    "    Discount2 = []\n",
    "    for i in discount2:\n",
    "        Discount2.append(i)\n",
    "\n",
    "    # Appending lists of all the pages and scrutinizing it appropriately.\n",
    "    BRAND = Brand2+Brand1+Brand2\n",
    "    PRODUCT_DESCRIPTION = Product_Description+Product_Description1+Product_Description2\n",
    "    PRICE = Price[0:40]+Price1[0:40]+Price2[0:40]\n",
    "    DISCOUNT = Discount[0:40]+Discount1[0:40]+Discount2[0:40]\n",
    "    \n",
    "    # Creating a dataframe to store the scrapped information\n",
    "    Sunglasses = pd.DataFrame({'BRAND' : BRAND[0:100], 'PRODUCT_DESCRIPTION' : PRODUCT_DESCRIPTION[0:100], 'PRICE' : PRICE[0:100], 'DISCOUNT' : DISCOUNT[0:100]})\n",
    "\n",
    "    # Returning the dataframe\n",
    "    return Sunglasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BRAND</th>\n",
       "      <th>PRODUCT_DESCRIPTION</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>DISCOUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>winsome</td>\n",
       "      <td>UV Protection Round Sunglasses (52)</td>\n",
       "      <td>₹1,039</td>\n",
       "      <td>[[20% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tazzx</td>\n",
       "      <td>UV Protection Round Sunglasses (52)</td>\n",
       "      <td>₹1,039</td>\n",
       "      <td>[[20% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spexra</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹674</td>\n",
       "      <td>[[25% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹312</td>\n",
       "      <td>[[87% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MAXX</td>\n",
       "      <td>Gradient, Mirrored, UV Protection Aviator Sung...</td>\n",
       "      <td>₹199</td>\n",
       "      <td>[[86% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>funglasses</td>\n",
       "      <td>Riding Glasses, Night Vision Round, Round Sung...</td>\n",
       "      <td>₹169</td>\n",
       "      <td>[[83% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Aviator Sunglasses (Free Size)</td>\n",
       "      <td>₹674</td>\n",
       "      <td>[[25% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Trendy Glasses</td>\n",
       "      <td>Mirrored, UV Protection Retro Square Sunglasse...</td>\n",
       "      <td>₹289</td>\n",
       "      <td>[[85% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>NuVew</td>\n",
       "      <td>UV Protection, Mirrored, Night Vision, Riding ...</td>\n",
       "      <td>₹282</td>\n",
       "      <td>[[69% off]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Pierre Cardin</td>\n",
       "      <td>Mirrored Aviator Sunglasses (59)</td>\n",
       "      <td>₹2,340</td>\n",
       "      <td>[[55% off]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             BRAND                                PRODUCT_DESCRIPTION   PRICE  \\\n",
       "0          winsome                UV Protection Round Sunglasses (52)  ₹1,039   \n",
       "1            Tazzx                UV Protection Round Sunglasses (52)  ₹1,039   \n",
       "2           Spexra      UV Protection Wayfarer Sunglasses (Free Size)    ₹674   \n",
       "3         Fastrack                UV Protection Round Sunglasses (54)    ₹312   \n",
       "4             MAXX  Gradient, Mirrored, UV Protection Aviator Sung...    ₹199   \n",
       "..             ...                                                ...     ...   \n",
       "95      funglasses  Riding Glasses, Night Vision Round, Round Sung...    ₹169   \n",
       "96        Fastrack       UV Protection Aviator Sunglasses (Free Size)    ₹674   \n",
       "97  Trendy Glasses  Mirrored, UV Protection Retro Square Sunglasse...    ₹289   \n",
       "98           NuVew  UV Protection, Mirrored, Night Vision, Riding ...    ₹282   \n",
       "99   Pierre Cardin                   Mirrored Aviator Sunglasses (59)  ₹2,340   \n",
       "\n",
       "       DISCOUNT  \n",
       "0   [[20% off]]  \n",
       "1   [[20% off]]  \n",
       "2   [[25% off]]  \n",
       "3   [[87% off]]  \n",
       "4   [[86% off]]  \n",
       "..          ...  \n",
       "95  [[83% off]]  \n",
       "96  [[25% off]]  \n",
       "97  [[85% off]]  \n",
       "98  [[69% off]]  \n",
       "99  [[55% off]]  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to\n",
    "go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includesearpods-poweradapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKC\n",
    "TSVZAXUHGREPBFGI&marketplace.\n",
    "When you will open the above link you will reach to the below shown webpage.\n",
    "As shown in the above page you have to scrape the tick marked attributes.\n",
    "\n",
    "These are\n",
    "1. Rating,\n",
    "2. Review_summary,\n",
    "3. Full review,\n",
    "\n",
    "You have to scrape this data for first 100 reviews.\n",
    "\n",
    "Step 1 : Find the URL that we wish to Scrape\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace\n",
    "\n",
    "Step 2 : Find the data you want to extract\n",
    "Rating,\n",
    "Review_summary,\n",
    "Full review.\n",
    "\n",
    "Step 3 : Write the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q7():\n",
    "\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "\n",
    "    # Configure the webdriver\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    \n",
    "    rating = soup.find_all('div', class_ = 'hGSR34 E_uFuv')\n",
    "\n",
    "    Ratings = []\n",
    "\n",
    "    for i in rating:\n",
    "        Ratings.append(i.text)\n",
    "        \n",
    "        \n",
    "    rev_sum = soup.find_all('p', class_ = '_2xg6Ul')\n",
    "    Rev_Sum = []\n",
    "    for i in rev_sum:\n",
    "        Rev_Sum.append(i.text)\n",
    "        \n",
    "    rev = soup.find_all('div', class_ = 'qwjRop')\n",
    "    Review = []\n",
    "    for i in rev:\n",
    "        Review.append(i.text)\n",
    "        \n",
    "        \n",
    "    driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=2\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup2 = BeautifulSoup(content)\n",
    "\n",
    "\n",
    "    driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=3\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup3 = BeautifulSoup(content)\n",
    "        \n",
    "\n",
    "    driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=4\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup4 = BeautifulSoup(content)\n",
    "    \n",
    "    \n",
    "    driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=5\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup5 = BeautifulSoup(content)\n",
    "    \n",
    "    \n",
    "    driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=6\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup6 = BeautifulSoup(content)\n",
    "    \n",
    "    \n",
    "    driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=7\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup7 = BeautifulSoup(content)\n",
    "    \n",
    "    \n",
    "    driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=8\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup8 = BeautifulSoup(content)\n",
    "    \n",
    "    \n",
    "    driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=9\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup9 = BeautifulSoup(content)\n",
    "    \n",
    "    \n",
    "    driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace=FLIPKART&page=10\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup10 = BeautifulSoup(content)\n",
    "    \n",
    "    \n",
    "    rating2 = soup2.find_all('div', class_ = 'hGSR34 E_uFuv')\n",
    "    Ratings2 = []\n",
    "    for i in rating2:\n",
    "        Ratings2.append(i.text)\n",
    "    Ratings2.insert(-1, 1)\n",
    "\n",
    "    rev_sum2 = soup2.find_all('p', class_ = '_2xg6Ul')\n",
    "    Rev_Sum2 = []\n",
    "    for i in rev_sum2:\n",
    "        Rev_Sum2.append(i.text)\n",
    "\n",
    "    rev2 = soup2.find_all('div', class_ = 'qwjRop')\n",
    "    Review2 = []\n",
    "    for i in rev2:\n",
    "        Review2.append(i.text)\n",
    "\n",
    "        \n",
    "    rating3 = soup3.find_all('div', class_ = 'hGSR34 E_uFuv')\n",
    "    Ratings3 = []\n",
    "    for i in rating3:\n",
    "        Ratings3.append(i.text)\n",
    "\n",
    "    rev_sum3 = soup3.find_all('p', class_ = '_2xg6Ul')\n",
    "    Rev_Sum3 = []\n",
    "    for i in rev_sum3:\n",
    "        Rev_Sum3.append(i.text)\n",
    "\n",
    "    rev3 = soup3.find_all('div', class_ = 'qwjRop')\n",
    "    Review3 = []\n",
    "    for i in rev3:\n",
    "        Review3.append(i.text)\n",
    "        \n",
    "        \n",
    "        \n",
    "    rating4 = soup4.find_all('div', class_ = 'hGSR34 E_uFuv')\n",
    "    Ratings4 = []\n",
    "    for i in rating4:\n",
    "        Ratings4.append(i.text)\n",
    "\n",
    "    rev_sum4 = soup4.find_all('p', class_ = '_2xg6Ul')\n",
    "    Rev_Sum4 = []\n",
    "    for i in rev_sum4:\n",
    "        Rev_Sum4.append(i.text)\n",
    "\n",
    "    rev4 = soup4.find_all('div', class_ = 'qwjRop')\n",
    "    Review4 = []\n",
    "    for i in rev4:\n",
    "        Review4.append(i.text) \n",
    "        \n",
    "        \n",
    "        \n",
    "    rating5 = soup5.find_all('div', class_ = 'hGSR34 E_uFuv')\n",
    "    Ratings5 = []\n",
    "    for i in rating5:\n",
    "        Ratings5.append(i.text)\n",
    "\n",
    "    rev_sum5 = soup5.find_all('p', class_ = '_2xg6Ul')\n",
    "    Rev_Sum5 = []\n",
    "    for i in rev_sum5:\n",
    "        Rev_Sum5.append(i.text)\n",
    "\n",
    "    rev5 = soup5.find_all('div', class_ = 'qwjRop')\n",
    "    Review5 = []\n",
    "    for i in rev5:\n",
    "        Review5.append(i.text)\n",
    "        \n",
    "        \n",
    "    rating6 = soup6.find_all('div', class_ = 'hGSR34 E_uFuv')\n",
    "    Ratings6 = []\n",
    "    for i in rating6:\n",
    "        Ratings6.append(i.text)\n",
    "\n",
    "    rev_sum6 = soup6.find_all('p', class_ = '_2xg6Ul')\n",
    "    Rev_Sum6 = []\n",
    "    for i in rev_sum6:\n",
    "        Rev_Sum6.append(i.text)\n",
    "\n",
    "    rev6 = soup6.find_all('div', class_ = 'qwjRop')\n",
    "    Review6 = []\n",
    "    for i in rev6:\n",
    "        Review6.append(i.text)\n",
    "        \n",
    "        \n",
    "\n",
    "    rating7 = soup7.find_all('div', class_ = 'hGSR34 E_uFuv')\n",
    "    Ratings7 = []\n",
    "    for i in rating7:\n",
    "        Ratings7.append(i.text)\n",
    "\n",
    "    rev_sum7 = soup7.find_all('p', class_ = '_2xg6Ul')\n",
    "    Rev_Sum7 = []\n",
    "    for i in rev_sum7:\n",
    "        Rev_Sum7.append(i.text)\n",
    "\n",
    "    rev7 = soup7.find_all('div', class_ = 'qwjRop')\n",
    "    Review7 = []\n",
    "    for i in rev7:\n",
    "        Review7.append(i.text)\n",
    "        \n",
    "        \n",
    "    rating8 = soup8.find_all('div', class_ = 'hGSR34 E_uFuv')\n",
    "    Ratings8 = []\n",
    "    for i in rating8:\n",
    "        Ratings8.append(i.text)\n",
    "\n",
    "    rev_sum8 = soup8.find_all('p', class_ = '_2xg6Ul')\n",
    "    Rev_Sum8 = []\n",
    "    for i in rev_sum8:\n",
    "        Rev_Sum8.append(i.text)\n",
    "\n",
    "    rev8 = soup8.find_all('div', class_ = 'qwjRop')\n",
    "    Review8 = []\n",
    "    for i in rev8:\n",
    "        Review8.append(i.text)\n",
    "        \n",
    "        \n",
    "    rating9 = soup9.find_all('div', class_ = 'hGSR34 E_uFuv')\n",
    "    Ratings9 = []\n",
    "    for i in rating9:\n",
    "        Ratings9.append(i.text)\n",
    "\n",
    "    rev_sum9 = soup9.find_all('p', class_ = '_2xg6Ul')\n",
    "    Rev_Sum9 = []\n",
    "    for i in rev_sum9:\n",
    "        Rev_Sum9.append(i.text)\n",
    "\n",
    "    rev9 = soup9.find_all('div', class_ = 'qwjRop')\n",
    "    Review9 = []\n",
    "    for i in rev9:\n",
    "        Review9.append(i.text)\n",
    "        \n",
    "        \n",
    "    rating10 = soup10.find_all('div', class_ = 'hGSR34 E_uFuv')\n",
    "    Ratings10 = []\n",
    "    for i in rating10:\n",
    "        Ratings10.append(i.text)\n",
    "\n",
    "    rev_sum10 = soup10.find_all('p', class_ = '_2xg6Ul')\n",
    "    Rev_Sum10 = []\n",
    "    for i in rev_sum10:\n",
    "        Rev_Sum10.append(i.text)\n",
    "\n",
    "    rev10 = soup10.find_all('div', class_ = 'qwjRop')\n",
    "    Review10 = []\n",
    "    for i in rev10:\n",
    "        Review10.append(i.text)\n",
    "        \n",
    "        \n",
    "    RATING = Ratings + Ratings2 + Ratings3 + Ratings4 + Ratings5 + Ratings6 + Ratings7 + Ratings8 + Ratings9 + Ratings10\n",
    "    SUMMARY_REVIEW = Rev_Sum + Rev_Sum2 + Rev_Sum3 + Rev_Sum4 + Rev_Sum5 + Rev_Sum6 + Rev_Sum7 + Rev_Sum8 + Rev_Sum9 + Rev_Sum10\n",
    "    FULL_REVIEW = Review + Review2 + Review3 + Review4 + Review5 + Review6 + Review7 + Review8 + Review9 + Review10\n",
    "    \n",
    "    \n",
    "    # Creating a dataframe to store the scrapped information\n",
    "    Reviews_Data = pd.DataFrame({'RATING' : RATING, 'SUMMARY_REVIEW' : SUMMARY_REVIEW, 'FULL_REVIEW' : FULL_REVIEW})\n",
    "\n",
    "    # Returning the dataframe\n",
    "    return Reviews_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RATING</th>\n",
       "      <th>SUMMARY_REVIEW</th>\n",
       "      <th>FULL_REVIEW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Amazing Powerful and Durable Gadget.I’m am ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>It’s a must buy who is looking for an upgrade ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>iphone 11 is a very good phone to buy only if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Value for money❤️❤️Its awesome mobile phone in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Terrific!!! Lucky to get this phone in first l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>3</td>\n",
       "      <td>Nice</td>\n",
       "      <td>Iphone 11 black 64gb is really a cool phone 1....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>3</td>\n",
       "      <td>Nice</td>\n",
       "      <td>Although it’s an iPhone, it doesn’t give anyth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Just wow!</td>\n",
       "      <td>Apple i Phone is the best phone available in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>use outside gives a outstanding experience ......</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RATING      SUMMARY_REVIEW  \\\n",
       "0       5    Perfect product!   \n",
       "1       5       Great product   \n",
       "2       5    Perfect product!   \n",
       "3       5  Highly recommended   \n",
       "4       5    Perfect product!   \n",
       "..    ...                 ...   \n",
       "95      5       Great product   \n",
       "96      3                Nice   \n",
       "97      3                Nice   \n",
       "98      5           Just wow!   \n",
       "99      5           Brilliant   \n",
       "\n",
       "                                          FULL_REVIEW  \n",
       "0   Amazing phone with great cameras and better ba...  \n",
       "1   Amazing Powerful and Durable Gadget.I’m am ver...  \n",
       "2   It’s a must buy who is looking for an upgrade ...  \n",
       "3   iphone 11 is a very good phone to buy only if ...  \n",
       "4   Value for money❤️❤️Its awesome mobile phone in...  \n",
       "..                                                ...  \n",
       "95  Terrific!!! Lucky to get this phone in first l...  \n",
       "96  Iphone 11 black 64gb is really a cool phone 1....  \n",
       "97  Although it’s an iPhone, it doesn’t give anyth...  \n",
       "98  Apple i Phone is the best phone available in t...  \n",
       "99  use outside gives a outstanding experience ......  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q7()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8: Scrape data for first 100 sneakers you find when you visit flipkart.com and\n",
    "search for “sneakers” in the search field.\n",
    "You have to scrape 4 attributes of each sneaker :\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. discount %\n",
    "As shown in the below image, you have to scrape the tick marked attributes.\n",
    "Also note that all the steps required during scraping should be done through code\n",
    "only and not manually.\n",
    "\n",
    "Step 1 : Find the URL that we wish to Scrape\n",
    "https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\n",
    "\n",
    "Step 2 : Find the data you want to extract\n",
    "Brand,\n",
    "Product Description,\n",
    "Price,\n",
    "discount %.\n",
    "\n",
    "Step 3 : Write the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q8():\n",
    "    \n",
    "    # Importing essential libraries\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "    \n",
    "    # Configure the webdriver\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(\"https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    # Scrapping the brand name - page 1\n",
    "    brand = soup.find_all('div', class_ = '_2B_pmu')\n",
    "    Brand = []\n",
    "    for i in brand:\n",
    "        Brand.append(i.text)\n",
    "        \n",
    "    # Scrapping the desc - page 1\n",
    "    desc = soup.find_all('a', class_ = '_2mylT6')\n",
    "    Product_Description = []\n",
    "    for i in desc:\n",
    "        Product_Description.append(i.text)\n",
    "        \n",
    "    # Scrapping the price - page 1\n",
    "    price = soup.find_all('div', class_ = '_1vC4OE')[0:40]\n",
    "    Price = []\n",
    "    for i in price:\n",
    "        Price.append(i.text)\n",
    "\n",
    "    # Scrapping the discounts - page 1\n",
    "    discount = soup.find_all('div', class_ = 'VGWI6T')[0:40]\n",
    "    Discount = []\n",
    "    for i in discount:\n",
    "        Discount.append(i.text)\n",
    "      \n",
    "    # Getting drivers for page 2\n",
    "    driver.get(\"https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page=2\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup2 = BeautifulSoup(content)\n",
    "    \n",
    "    # Scrapping the brand - page 2\n",
    "    brand2 = soup2.find_all('div', class_ = '_2B_pmu')\n",
    "    Brand2 = []\n",
    "    for i in brand2:\n",
    "        Brand2.append(i.text)\n",
    "\n",
    "    # Scrapping the sescription - page 2\n",
    "    desc2 = soup2.find_all('a', class_ = '_2mylT6')\n",
    "    Product_Description2 = []\n",
    "    for i in desc2:\n",
    "        Product_Description2.append(i.text)\n",
    "\n",
    "    # Scrapping the price - page 2\n",
    "    price2 = soup2.find_all('div', class_ = '_1vC4OE')[0:40]\n",
    "    Price2 = []\n",
    "    for i in price2:\n",
    "        Price2.append(i.text)\n",
    "\n",
    "    # Scrappig the discounts - page 2\n",
    "    discount2 = soup2.find_all('div', class_ = 'VGWI6T')[0:40]\n",
    "    Discount2 = []\n",
    "    for i in discount2:\n",
    "        Discount2.append(i.text)\n",
    "        \n",
    "    # Getting the drivers for page 3 \n",
    "    driver.get(\"https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page=3\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup3 = BeautifulSoup(content)\n",
    "    \n",
    "    # Scrapping the brand for page 3\n",
    "    brand3 = soup3.find_all('div', class_ = '_2B_pmu')\n",
    "    Brand3 = []\n",
    "    for i in brand3:\n",
    "        Brand3.append(i.text)\n",
    "\n",
    "    # Scrapping description for page 3\n",
    "    desc3 = soup3.find_all('a', class_ = '_2mylT6')\n",
    "    Product_Description3 = []\n",
    "    for i in desc3:\n",
    "        Product_Description3.append(i.text)\n",
    "\n",
    "    # Scrapping price for page 3\n",
    "    price3 = soup3.find_all('div', class_ = '_1vC4OE')[0:40]\n",
    "    Price3 = []\n",
    "    for i in price3:\n",
    "        Price3.append(i.text)\n",
    "\n",
    "    # Scrapping discounts for page 3\n",
    "    discount3 = soup3.find_all('div', class_ = 'VGWI6T')[0:40]\n",
    "    Discount3 = []\n",
    "    for i in discount3:\n",
    "        Discount3.append(i.text)\n",
    "        \n",
    "    # Appending \n",
    "    BRAND = Brand + Brand2 + Brand3\n",
    "    PRODUCT_DESCRIPTION = Product_Description + Product_Description2 + Product_Description3\n",
    "    PRICE = Price + Price2 + Price3\n",
    "    DISCOUNT = Discount + Discount2 + Discount3\n",
    "    \n",
    "    \n",
    "    # Creating a dataframe to store the scrapped information\n",
    "    Sneakers_Data = pd.DataFrame({'BRAND' : BRAND[0:100], 'PRODUCT_DESCRIPTION' : PRODUCT_DESCRIPTION[0:100], 'PRICE' : PRICE[0:100], 'DISCOUNT' : DISCOUNT[0:100]})\n",
    "\n",
    "    # Returning the dataframe\n",
    "    return Sneakers_Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BRAND</th>\n",
       "      <th>PRODUCT_DESCRIPTION</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>DISCOUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Red Tape</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹1,023</td>\n",
       "      <td>75% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Puma</td>\n",
       "      <td>Puma Smash v2 Sneakers For Men</td>\n",
       "      <td>₹1,999</td>\n",
       "      <td>50% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Combo Pack of 4 Casual Sneakers With Sneakers ...</td>\n",
       "      <td>₹461</td>\n",
       "      <td>76% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>171 Smart Tan Lace-Ups Casuals for Men Sneaker...</td>\n",
       "      <td>₹236</td>\n",
       "      <td>52% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Latest Fashion Combo Pack of 2 Pairs Sneakers ...</td>\n",
       "      <td>₹525</td>\n",
       "      <td>64% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Puma</td>\n",
       "      <td>Icon IDP Sneakers For Men</td>\n",
       "      <td>₹1,399</td>\n",
       "      <td>57% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Freedom Daisy</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹424</td>\n",
       "      <td>57% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Believe</td>\n",
       "      <td>Sneakers for men(blue_6) Sneakers For Men</td>\n",
       "      <td>₹399</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Camfoot</td>\n",
       "      <td>Combo Pack of 2 Latest Collection Stylish Casu...</td>\n",
       "      <td>₹328</td>\n",
       "      <td>67% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Red Tape</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹999</td>\n",
       "      <td>75% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            BRAND                                PRODUCT_DESCRIPTION   PRICE  \\\n",
       "0        Red Tape                                   Sneakers For Men  ₹1,023   \n",
       "1            Puma                     Puma Smash v2 Sneakers For Men  ₹1,999   \n",
       "2          Chevit  Combo Pack of 4 Casual Sneakers With Sneakers ...    ₹461   \n",
       "3          Chevit  171 Smart Tan Lace-Ups Casuals for Men Sneaker...    ₹236   \n",
       "4          Chevit  Latest Fashion Combo Pack of 2 Pairs Sneakers ...    ₹525   \n",
       "..            ...                                                ...     ...   \n",
       "95           Puma                          Icon IDP Sneakers For Men  ₹1,399   \n",
       "96  Freedom Daisy                                   Sneakers For Men    ₹424   \n",
       "97        Believe          Sneakers for men(blue_6) Sneakers For Men    ₹399   \n",
       "98        Camfoot  Combo Pack of 2 Latest Collection Stylish Casu...    ₹328   \n",
       "99       Red Tape                                   Sneakers For Men    ₹999   \n",
       "\n",
       "   DISCOUNT  \n",
       "0   75% off  \n",
       "1   50% off  \n",
       "2   76% off  \n",
       "3   52% off  \n",
       "4   64% off  \n",
       "..      ...  \n",
       "95  57% off  \n",
       "96  57% off  \n",
       "97  60% off  \n",
       "98  67% off  \n",
       "99  75% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q8()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9: Go to the link - https://www.myntra.com/shoes\n",
    "Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black”, as shown in\n",
    "the below image.\n",
    "And then scrape First 100 shoes data you get. The data should include “Brand” of\n",
    "the shoes , Short Shoe description, price of the shoe as shown in the below image.\n",
    "Please note that applying the filter and scraping the data , everything should be\n",
    "done through code only and there should not be any manual step.\n",
    "\n",
    "Step 1 : Find the URL that we wish to Scrape\n",
    "https://www.myntra.com/shoes?f=Color%3ABlack_36454f&rf=Price%3A6649.0_13099.0_6649.0%20TO%2013099.0\n",
    "\n",
    "Step 2 : Find the data you want to extract\n",
    "Brand of the shoes,\n",
    "Short Shoe description,\n",
    "price of the shoe.\n",
    "\n",
    "Step 3 : Write the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q9():\n",
    "    \n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "    # Configure the webdriver\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(\"https://www.myntra.com/shoes?f=Color%3ABlack_36454f&rf=Price%3A6649.0_13099.0_6649.0%20TO%2013099.0\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)\n",
    "    \n",
    "    \n",
    "    brand = soup.find_all('h3', class_ = 'product-brand')\n",
    "    Brand = []\n",
    "    for i in brand:\n",
    "        Brand.append(i.text)\n",
    "    len(Brand)\n",
    "    \n",
    "    \n",
    "    desc = soup.find_all('h4', class_ = 'product-product')\n",
    "    Description = []\n",
    "    for i in desc:\n",
    "        Description.append(i.text)\n",
    "    len(Description)\n",
    "    \n",
    "    \n",
    "    price = soup.find_all('div', class_ = 'product-price')\n",
    "    Price = []\n",
    "    for i in price:\n",
    "        Price.append(i.span.text)\n",
    "    len(Price)\n",
    "    \n",
    "    \n",
    "    driver.get(\"https://www.myntra.com/shoes?f=Color%3ABlack_36454f&p=2&rf=Price%3A6649.0_13099.0_6649.0%20TO%2013099.0\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup2 = BeautifulSoup(content)\n",
    "    \n",
    "    \n",
    "    brand2 = soup2.find_all('h3', class_ = 'product-brand')\n",
    "    Brand2 = []\n",
    "    for i in brand2:\n",
    "        Brand2.append(i.text)\n",
    "    len(Brand2)\n",
    "    \n",
    "    \n",
    "    desc2 = soup2.find_all('h4', class_ = 'product-product')\n",
    "    Description2 = []\n",
    "    for i in desc2:\n",
    "        Description2.append(i.text)\n",
    "    len(Description2)\n",
    "    \n",
    "    \n",
    "    price2 = soup2.find_all('div', class_ = 'product-price')\n",
    "    Price2 = []\n",
    "    for i in price2:\n",
    "        Price2.append(i.span.text)\n",
    "    len(Price2)\n",
    "    \n",
    "    \n",
    "    BRAND = Brand + Brand2\n",
    "    DESCRIPTION = Description + Description2\n",
    "    PRICE = Price + Price2\n",
    "    \n",
    "    \n",
    "    # Creating a dataframe to store the scrapped information\n",
    "    Shoes_Data = pd.DataFrame({'BRAND' : BRAND[0:100], 'DESCRIPTION' : DESCRIPTION[0:100], 'PRICE' : PRICE[0:100]})\n",
    "    \n",
    "\n",
    "    # Returning the dataframe\n",
    "    return Shoes_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BRAND</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Puma</td>\n",
       "      <td>Trailfox Overland MTS Grid Run</td>\n",
       "      <td>Rs. 6999Rs. 9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men Zoom Running Shoes</td>\n",
       "      <td>Rs. 7995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hush Puppies</td>\n",
       "      <td>Men Solid Leather Formal Slip-Ons</td>\n",
       "      <td>Rs. 7649Rs. 8999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADIDAS</td>\n",
       "      <td>Men FluidFlow Running Shoes</td>\n",
       "      <td>Rs. 7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men Zoom Running Shoes</td>\n",
       "      <td>Rs. 7995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>New Balance</td>\n",
       "      <td>Men LazrV2 HypoKnit Run Shoes</td>\n",
       "      <td>Rs. 7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>New Balance</td>\n",
       "      <td>Tekela Magique Football Shoes</td>\n",
       "      <td>Rs. 8499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Clarks</td>\n",
       "      <td>Men Leather Formal Loafers</td>\n",
       "      <td>Rs. 6999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Heel &amp; Buckle London</td>\n",
       "      <td>Men Leather Sneakers</td>\n",
       "      <td>Rs. 6993Rs. 9990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>FILA</td>\n",
       "      <td>Men Solid Sneakers</td>\n",
       "      <td>Rs. 7499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   BRAND                        DESCRIPTION             PRICE\n",
       "0                   Puma     Trailfox Overland MTS Grid Run  Rs. 6999Rs. 9999\n",
       "1                   Nike             Men Zoom Running Shoes          Rs. 7995\n",
       "2           Hush Puppies  Men Solid Leather Formal Slip-Ons  Rs. 7649Rs. 8999\n",
       "3                 ADIDAS        Men FluidFlow Running Shoes          Rs. 7999\n",
       "4                   Nike             Men Zoom Running Shoes          Rs. 7995\n",
       "..                   ...                                ...               ...\n",
       "95           New Balance      Men LazrV2 HypoKnit Run Shoes          Rs. 7999\n",
       "96           New Balance      Tekela Magique Football Shoes          Rs. 8499\n",
       "97                Clarks         Men Leather Formal Loafers          Rs. 6999\n",
       "98  Heel & Buckle London               Men Leather Sneakers  Rs. 6993Rs. 9990\n",
       "99                  FILA                 Men Solid Sneakers          Rs. 7499\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q9()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10: Go to webpage https://www.amazon.in/\n",
    " Enter “Laptop” in the search field and then click the search icon.\n",
    " Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” as shown in the\n",
    "below image:\n",
    "    \n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes\n",
    "for each laptop:\n",
    "1. title\n",
    "2. Ratings\n",
    "3. Price\n",
    "As shown in the below image as the tick marked attributes.\n",
    "\n",
    "Step 1 : Find the URL that we wish to Scrape\n",
    "https://www.amazon.in/s?k=Laptop&i=computers&rh=n%3A1375424031%2Cp_n_feature_thirteen_browse-bin%3A12598163031%7C16757432031&dc&qid=1604585877&rnid=12598141031&ref=sr_nr_p_n_feature_thirteen_browse-bin_17\n",
    "\n",
    "Step 2 : Find the data you want to extract\n",
    "title,\n",
    "Ratings,\n",
    "Price.\n",
    "\n",
    "Step 3 : Write the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q10():\n",
    "    \n",
    "    # Importing essential libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    import requests\n",
    "    import urllib.request\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "    # Configure the webdriver\n",
    "    driver = webdriver.Chrome(\"chromedriver.exe\")\n",
    "    driver.get(\"https://www.amazon.in/s?k=Laptop&i=computers&rh=n%3A1375424031%2Cp_n_feature_thirteen_browse-bin%3A12598163031%7C16757432031&dc&qid=1604585877&rnid=12598141031&ref=sr_nr_p_n_feature_thirteen_browse-bin_17\")\n",
    "\n",
    "    # Creating soup\n",
    "    content = driver.page_source\n",
    "    soup = BeautifulSoup(content)  \n",
    "    \n",
    "    # Scraping title\n",
    "    title = soup.find_all('span', class_ = 'a-size-medium a-color-base a-text-normal')[0:10]\n",
    "    Title = []\n",
    "    for i in title:\n",
    "        Title.append(i.text)\n",
    "    \n",
    "    # Scraping Ratings\n",
    "    rating = soup.find_all('a', class_ = 'a-popover-trigger a-declarative')[0:10]\n",
    "    Ratings = []\n",
    "    for i in rating:\n",
    "        Ratings.append(i.text)\n",
    "\n",
    "    \n",
    "    # Scrapping the Price\n",
    "    price = soup.find_all('span', class_ = 'a-price-whole')[0:10]\n",
    "    Price = []\n",
    "    for i in price:\n",
    "        Price.append(i.text)\n",
    "    len(Price)\n",
    "    \n",
    "    \n",
    "   \n",
    "    # Creating a dataframe to store the scrapped information\n",
    "    Laptop_Data = pd.DataFrame({'Title' : Title[0:10], 'Ratings' : Ratings[0:10], 'Price' : Price[0:10]})\n",
    "\n",
    "    # Returning the dataframe\n",
    "    return Laptop_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HP Pavilion x360 Core i7 8th Gen 14-inch Touch...</td>\n",
       "      <td>4.1 out of 5 stars</td>\n",
       "      <td>81,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acer Nitro 5 Intel i7 - 9th Gen 17.3-inch Disp...</td>\n",
       "      <td>2.9 out of 5 stars</td>\n",
       "      <td>59,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Renewed) Dell Latitude E5570 15-inch Laptop (...</td>\n",
       "      <td>3.5 out of 5 stars</td>\n",
       "      <td>61,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dell XPS 7590 15.6-inch UHD Display Laptop (9t...</td>\n",
       "      <td>2.9 out of 5 stars</td>\n",
       "      <td>2,33,000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dell Alienware m15(R3) 15.6-inch FHD Gaming La...</td>\n",
       "      <td>4.4 out of 5 stars</td>\n",
       "      <td>1,99,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HP 14s dr1006TU 14-inch Laptop (10th Gen Core ...</td>\n",
       "      <td>3.6 out of 5 stars</td>\n",
       "      <td>39,999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(Renewed) Dell Latitude E7240 12.5-inch Laptop...</td>\n",
       "      <td>2.2 out of 5 stars</td>\n",
       "      <td>49,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(Renewed) HP EliteBook 1030 G2 X 360 Notebook ...</td>\n",
       "      <td>4.1 out of 5 stars</td>\n",
       "      <td>81,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HP Pavilion x360 Core i7 8th Gen 14-inch Touch...</td>\n",
       "      <td>2.6 out of 5 stars</td>\n",
       "      <td>49,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(Renewed) HP EliteBook X360 1030 G2 Laptop (Co...</td>\n",
       "      <td>4.4 out of 5 stars</td>\n",
       "      <td>1,38,000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title             Ratings  \\\n",
       "0  HP Pavilion x360 Core i7 8th Gen 14-inch Touch...  4.1 out of 5 stars   \n",
       "1  Acer Nitro 5 Intel i7 - 9th Gen 17.3-inch Disp...  2.9 out of 5 stars   \n",
       "2  (Renewed) Dell Latitude E5570 15-inch Laptop (...  3.5 out of 5 stars   \n",
       "3  Dell XPS 7590 15.6-inch UHD Display Laptop (9t...  2.9 out of 5 stars   \n",
       "4  Dell Alienware m15(R3) 15.6-inch FHD Gaming La...  4.4 out of 5 stars   \n",
       "5  HP 14s dr1006TU 14-inch Laptop (10th Gen Core ...  3.6 out of 5 stars   \n",
       "6  (Renewed) Dell Latitude E7240 12.5-inch Laptop...  2.2 out of 5 stars   \n",
       "7  (Renewed) HP EliteBook 1030 G2 X 360 Notebook ...  4.1 out of 5 stars   \n",
       "8  HP Pavilion x360 Core i7 8th Gen 14-inch Touch...  2.6 out of 5 stars   \n",
       "9  (Renewed) HP EliteBook X360 1030 G2 Laptop (Co...  4.4 out of 5 stars   \n",
       "\n",
       "      Price  \n",
       "0    81,990  \n",
       "1    59,990  \n",
       "2    61,999  \n",
       "3  2,33,000  \n",
       "4  1,99,990  \n",
       "5    39,999  \n",
       "6    49,990  \n",
       "7    81,990  \n",
       "8    49,990  \n",
       "9  1,38,000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q10()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
